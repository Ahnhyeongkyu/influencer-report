"""
ì¸í”Œë£¨ì–¸ì„œ ìº í˜ì¸ ì„±ê³¼ ë¦¬í¬íŠ¸ - Streamlit ì›¹ ì•±

ë©€í‹° í”Œë«í¼ í¬ë¡¤ë§ ë° ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
v2.0 - í”Œë«í¼ ì¿ í‚¤ ì¸ì¦ ì§€ì›
"""

import sys
import os
import json
import logging
import platform as sys_platform
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
import time

import streamlit as st
import pandas as pd

# í™˜ê²½ ê°ì§€ (Cloud vs Local)
IS_CLOUD = sys_platform.system() == "Linux" and os.path.exists("/etc/debian_version")
IS_LOCAL = not IS_CLOUD

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from src.auth import is_authenticated, show_login_form, show_user_info, init_session_state
from src.platform_auth import (
    init_platform_auth_state,
    render_platform_auth_section,
    get_platform_cookies,
    is_platform_authenticated,
)
from src.utils.url_parser import (
    parse_urls,
    parse_csv_urls,
    count_by_platform,
    get_platform_display_name,
    PLATFORM_DISPLAY_NAMES,
)
from src.utils.data_processor import (
    aggregate_results,
    group_by_platform,
    calculate_campaign_metrics,
    export_to_dataframe,
    generate_summary_table,
    format_number,
)

# í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
from src.crawlers import (
    crawl_xhs_post,
    crawl_xhs_posts,
    crawl_youtube_video,
    crawl_youtube_videos,
    crawl_instagram_post,
    crawl_instagram_posts,
    crawl_facebook_post,
    crawl_facebook_posts,
    crawl_dcard_post,
    crawl_dcard_posts,
)

# ì¡°íšŒìˆ˜ ìˆ˜ì§‘ ê°€ëŠ¥ í”Œë«í¼ (True = ìˆ˜ì§‘ ê°€ëŠ¥, False = ìˆ˜ì§‘ ë¶ˆê°€)
PLATFORM_VIEW_SUPPORT = {
    "youtube": True,       # YouTubeëŠ” ì¡°íšŒìˆ˜ ê³µê°œ
    "instagram": False,    # Instagram ì¼ë°˜ ê²Œì‹œë¬¼ì€ ì¡°íšŒìˆ˜ ë¹„ê³µê°œ (ë¦´ìŠ¤ë§Œ ì œê³µ)
    "facebook": False,     # Facebook ê³µê°œ í˜ì´ì§€ë„ ì¡°íšŒìˆ˜ ë¹„ê³µê°œ
    "xiaohongshu": False,  # ìƒ¤ì˜¤í™ìŠˆ ì¡°íšŒìˆ˜ ë¹„ê³µê°œ
    "dcard": False,        # Dcard ì¡°íšŒìˆ˜ ë¹„ê³µê°œ
}

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì¸í”Œë£¨ì–¸ì„œ ìº í˜ì¸ ì„±ê³¼ ë¦¬í¬íŠ¸",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ì»¤ìŠ¤í…€ CSS
st.markdown(
    """
    <style>
    .main-header {
        font-size: 2rem;
        font-weight: bold;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        font-size: 1rem;
        color: #666;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
    }
    .metric-value {
        font-size: 1.5rem;
        font-weight: bold;
        color: #1f77b4;
    }
    .metric-label {
        font-size: 0.9rem;
        color: #666;
    }
    .platform-badge {
        display: inline-block;
        padding: 0.25rem 0.5rem;
        border-radius: 0.25rem;
        font-size: 0.8rem;
        margin-right: 0.5rem;
    }
    .status-success {
        color: #28a745;
    }
    .status-error {
        color: #dc3545;
    }
    .status-pending {
        color: #ffc107;
    }
    </style>
    """,
    unsafe_allow_html=True,
)


def init_app_state():
    """ì•± ìƒíƒœ ì´ˆê¸°í™”"""
    init_session_state()
    init_platform_auth_state()

    if "campaign_info" not in st.session_state:
        st.session_state.campaign_info = {
            "name": "",
            "advertiser": "",
            "start_date": datetime.now().date(),
            "end_date": (datetime.now() + timedelta(days=30)).date(),
        }

    if "urls" not in st.session_state:
        st.session_state.urls = []

    if "crawl_results" not in st.session_state:
        st.session_state.crawl_results = []

    if "crawling_status" not in st.session_state:
        st.session_state.crawling_status = "idle"  # idle, running, completed, error


def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§ - ìº í˜ì¸ ì •ë³´ ì…ë ¥"""
    with st.sidebar:
        st.markdown("### ìº í˜ì¸ ì •ë³´")
        st.markdown("---")

        # ìº í˜ì¸ëª…
        st.session_state.campaign_info["name"] = st.text_input(
            "ìº í˜ì¸ëª…",
            value=st.session_state.campaign_info.get("name", ""),
            placeholder="ì˜ˆ: 2024 ì‹ ì œí’ˆ ëŸ°ì¹­ ìº í˜ì¸",
        )

        # ê´‘ê³ ì£¼ëª…
        st.session_state.campaign_info["advertiser"] = st.text_input(
            "ê´‘ê³ ì£¼ëª…",
            value=st.session_state.campaign_info.get("advertiser", ""),
            placeholder="ì˜ˆ: ABC ë¸Œëœë“œ",
        )

        # ìº í˜ì¸ ê¸°ê°„
        st.markdown("**ìº í˜ì¸ ê¸°ê°„**")
        col1, col2 = st.columns(2)
        with col1:
            st.session_state.campaign_info["start_date"] = st.date_input(
                "ì‹œì‘ì¼",
                value=st.session_state.campaign_info.get("start_date", datetime.now().date()),
            )
        with col2:
            st.session_state.campaign_info["end_date"] = st.date_input(
                "ì¢…ë£Œì¼",
                value=st.session_state.campaign_info.get("end_date", (datetime.now() + timedelta(days=30)).date()),
            )

        # ì§€ì› í”Œë«í¼ ì•ˆë‚´
        st.markdown("---")
        st.markdown("### ì§€ì› í”Œë«í¼")
        st.caption("âœ…=OK, ğŸ”‘=ì¿ í‚¤ì €ì¥(ë§Œë£Œê°€ëŠ¥), âš ï¸=ë¯¸ì„¤ì •")
        for platform, display_name in PLATFORM_DISPLAY_NAMES.items():
            # ì¸ì¦ ìƒíƒœ í‘œì‹œ
            if platform == "youtube":
                status = "âœ…"  # YouTubeëŠ” í•­ìƒ OK
            elif is_platform_authenticated(platform):
                status = "ğŸ”‘"  # ì¿ í‚¤ ì„¤ì •ë¨ (ë§Œë£Œ ê°€ëŠ¥)
            else:
                status = "âš ï¸"
            st.markdown(f"- {status} {display_name}")

        # í”Œë«í¼ ì¸ì¦ ì„¤ì •
        st.markdown("---")
        render_platform_auth_section()

        # ì¸ì¦ ëª¨ë“œ ì„¤ì • (ë¡œì»¬ í™˜ê²½ë§Œ)
        if IS_LOCAL:
            st.markdown("---")
            st.markdown("### í¬ë¡¤ë§ ì„¤ì •")
            st.session_state.auth_mode = st.checkbox(
                "ì¸ì¦ ëª¨ë“œ",
                value=st.session_state.get("auth_mode", False),
                help="í™œì„±í™”í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ ì—´ë ¤ QR ì½”ë“œ ìŠ¤ìº”ì´ë‚˜ Cloudflare ì¸ì¦ì„ ì§ì ‘ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ìƒ¤ì˜¤í™ìŠˆ, Dcard ë“±)"
            )
            if st.session_state.auth_mode:
                st.info(
                    "ì¸ì¦ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                    "í¬ë¡¤ë§ ì‹œì‘ ì‹œ ë¸Œë¼ìš°ì € ì°½ì´ ì—´ë¦½ë‹ˆë‹¤.\n"
                    "- ìƒ¤ì˜¤í™ìŠˆ: QR ì½”ë“œ ìŠ¤ìº”\n"
                    "- Dcard: Cloudflare ì¸ì¦ ì™„ë£Œ\n\n"
                    "ì¸ì¦ í›„ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤."
                )
        else:
            # Cloud í™˜ê²½ì—ì„œëŠ” auth_mode ë¹„í™œì„±í™”
            st.session_state.auth_mode = False

        # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
        show_user_info()


def render_url_input():
    """URL ì…ë ¥ ì„¹ì…˜ ë Œë”ë§"""
    st.markdown("### URL ì…ë ¥")

    # íƒ­ìœ¼ë¡œ ì…ë ¥ ë°©ì‹ ì„ íƒ
    tab1, tab2 = st.tabs(["ì§ì ‘ ì…ë ¥", "CSV ì—…ë¡œë“œ"])

    with tab1:
        st.markdown("ê²Œì‹œë¬¼ URLì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”.")
        url_text = st.text_area(
            "URL ëª©ë¡",
            height=200,
            placeholder="""https://www.youtube.com/watch?v=xxxxx
https://www.instagram.com/p/xxxxx
https://www.xiaohongshu.com/explore/xxxxx
https://www.facebook.com/xxxxx/posts/xxxxx
https://www.dcard.tw/f/xxx/p/xxxxx""",
            key="url_text_input",
        )

        if st.button("URL ë¶„ì„", key="analyze_urls"):
            if url_text.strip():
                parsed = parse_urls(url_text)
                st.session_state.urls = parsed
                st.success(f"{len(parsed)}ê°œ URLì´ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
            else:
                st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")

    with tab2:
        st.markdown("URLì´ í¬í•¨ëœ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        uploaded_file = st.file_uploader(
            "CSV íŒŒì¼ ì„ íƒ",
            type=["csv"],
            key="csv_upload",
        )

        if uploaded_file is not None:
            try:
                content = uploaded_file.read().decode("utf-8")
                parsed = parse_csv_urls(content)
                st.session_state.urls = parsed
                st.success(f"{len(parsed)}ê°œ URLì´ CSVì—ì„œ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
            except Exception as e:
                logger.error(f"CSV íŒŒì‹± ì˜¤ë¥˜: {e}")
                st.error("CSV íŒŒì¼ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


def render_url_preview():
    """URL ë¶„ì„ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°"""
    urls = st.session_state.get("urls", [])

    if not urls:
        return

    st.markdown("### URL ë¶„ì„ ê²°ê³¼")

    # í”Œë«í¼ë³„ ì¹´ìš´íŠ¸
    platform_counts = count_by_platform(urls)
    valid_count = sum(1 for u in urls if u.get("valid"))
    invalid_count = len(urls) - valid_count

    # ìš”ì•½ í‘œì‹œ
    cols = st.columns([1, 1, 1, 2])
    with cols[0]:
        st.metric("ì „ì²´ URL", len(urls))
    with cols[1]:
        st.metric("ìœ íš¨", valid_count)
    with cols[2]:
        st.metric("ì˜¤ë¥˜", invalid_count)
    with cols[3]:
        platform_text = " / ".join([
            f"{get_platform_display_name(p)}: {c}"
            for p, c in platform_counts.items()
        ])
        st.markdown(f"**í”Œë«í¼ë³„:** {platform_text}")

    # URL ìƒì„¸ ëª©ë¡
    with st.expander("URL ìƒì„¸ ëª©ë¡", expanded=True):
        for i, url_info in enumerate(urls):
            col1, col2, col3 = st.columns([4, 1, 1])
            with col1:
                st.text(url_info.get("url", ""))
            with col2:
                if url_info.get("valid"):
                    from html import escape as html_escape
                    st.markdown(f'<span class="status-success">{html_escape(get_platform_display_name(url_info.get("platform", "")))}</span>', unsafe_allow_html=True)
                else:
                    st.markdown(f'<span class="status-error">ì˜¤ë¥˜</span>', unsafe_allow_html=True)
            with col3:
                if url_info.get("error"):
                    st.error(url_info.get("error"))

    # í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
    st.markdown("---")
    if valid_count > 0:
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button(
                f"í¬ë¡¤ë§ ì‹œì‘ ({valid_count}ê°œ URL)",
                key="start_crawling",
                use_container_width=True,
                type="primary",
            ):
                st.session_state.crawling_status = "running"
                st.rerun()


def get_crawler_for_platform(platform: str):
    """í”Œë«í¼ì— ë§ëŠ” í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ë°˜í™˜"""
    crawlers = {
        "xiaohongshu": crawl_xhs_post,
        "youtube": crawl_youtube_video,
        "instagram": crawl_instagram_post,
        "facebook": crawl_facebook_post,
        "dcard": crawl_dcard_post,
    }
    return crawlers.get(platform)


def apply_cookies_to_session(session, cookies: dict, domain: str) -> None:
    """
    requests ì„¸ì…˜ì— ì¿ í‚¤ë¥¼ ë„ë©”ì¸ê³¼ í•¨ê»˜ ì ìš©

    ë‹¨ìˆœ dict.update()ëŠ” ë„ë©”ì¸ ì •ë³´ê°€ ì—†ì–´ì„œ ì„œë²„ê°€ ì¸ì¦ì„ ê±°ë¶€í•  ìˆ˜ ìˆìŒ.
    ëª…ì‹œì ìœ¼ë¡œ ë„ë©”ì¸ì„ ì„¤ì •í•´ì•¼ Instagram ë“± í”Œë«í¼ì—ì„œ ì¸ì¦ì´ ë™ì‘í•¨.

    Args:
        session: requests.Session ê°ì²´
        cookies: ì¿ í‚¤ ë”•ì…”ë„ˆë¦¬ {"name": "value"}
        domain: ì¿ í‚¤ ë„ë©”ì¸ (ì˜ˆ: ".instagram.com")
    """
    if not session or not cookies:
        return

    for name, value in cookies.items():
        if value:  # ê°’ì´ ìˆëŠ” ì¿ í‚¤ë§Œ ì„¤ì •
            session.cookies.set(
                name=name,
                value=value,
                domain=domain,
                path="/"
            )
    logger.info(f"ì¿ í‚¤ ì ìš© ì™„ë£Œ: {domain} - {list(cookies.keys())}")


# í”Œë«í¼ë³„ ë„ë©”ì¸ ë§¤í•‘
PLATFORM_DOMAINS = {
    "instagram": ".instagram.com",
    "facebook": ".facebook.com",
    "xiaohongshu": ".xiaohongshu.com",
    "dcard": ".dcard.tw",
}


def crawl_with_cookies(platform: str, url: str, auth_mode: bool = False) -> dict:
    """
    ì¿ í‚¤ë¥¼ í¬í•¨í•˜ì—¬ í¬ë¡¤ë§ ìˆ˜í–‰

    Args:
        platform: í”Œë«í¼ ì´ë¦„
        url: í¬ë¡¤ë§í•  URL
        auth_mode: ì¸ì¦ ëª¨ë“œ (Trueë©´ ë¸Œë¼ìš°ì € ì°½ í‘œì‹œí•˜ì—¬ QR/Cloudflare ì¸ì¦ ê°€ëŠ¥)

    Returns:
        í¬ë¡¤ë§ ê²°ê³¼
    """
    cookies = get_platform_cookies(platform)
    crawler = get_crawler_for_platform(platform)
    domain = PLATFORM_DOMAINS.get(platform, "")

    if not crawler:
        return {
            "platform": platform,
            "url": url,
            "error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í”Œë«í¼: {platform}",
            "crawled_at": datetime.now().isoformat(),
        }

    # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ê²°ì •:
    # - Cloud í™˜ê²½: í•­ìƒ headless (ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ ë¶ˆê°€)
    # - Local + ì¿ í‚¤ ìˆìŒ: headless (ì¸ì¦ ë¶ˆí•„ìš”)
    # - Local + ì¿ í‚¤ ì—†ìŒ + auth_mode: ë¹„headless (ì¸ì¦ í•„ìš”)
    # - Local + ì¿ í‚¤ ì—†ìŒ + not auth_mode: headless (API ì‹œë„)
    has_cookies = bool(cookies)

    if IS_CLOUD:
        use_headless = True  # CloudëŠ” í•­ìƒ headless
    elif auth_mode:
        use_headless = False  # ì¸ì¦ ëª¨ë“œë©´ ë¸Œë¼ìš°ì € í‘œì‹œ (ì¿ í‚¤ ìœ ë¬´ ë¬´ê´€)
        logger.info(f"ì¸ì¦ ëª¨ë“œ í™œì„±í™” - ë¸Œë¼ìš°ì € ì°½ì´ ì—´ë¦½ë‹ˆë‹¤ ({platform})")
    elif has_cookies:
        use_headless = True  # ì¿ í‚¤ ìˆìœ¼ë©´ headlessë¡œ ì¶©ë¶„
    else:
        use_headless = True  # ê¸°ë³¸ì€ headless

    try:
        # YouTubeëŠ” ì¿ í‚¤ ë¶ˆí•„ìš”
        if platform == "youtube":
            return crawler(url)

        # ë‹¤ë¥¸ í”Œë«í¼ì€ ì¿ í‚¤ì™€ í•¨ê»˜ í¬ë¡¤ë§
        if platform == "instagram":
            from src.crawlers.instagram_crawler import InstagramCrawler

            # ì‚¬ì´ë“œë°” ì¿ í‚¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (Selenium fallbackìš©)
            if cookies:
                cookie_file = Path(__file__).parent.parent / "data" / "cookies" / "instagram_cookies.json"
                cookie_file.parent.mkdir(parents=True, exist_ok=True)
                cookie_list = [
                    {"name": name, "value": value, "domain": ".instagram.com", "path": "/", "secure": True, "httpOnly": True}
                    for name, value in cookies.items()
                ]
                with open(cookie_file, "w", encoding="utf-8") as f:
                    json.dump(cookie_list, f, ensure_ascii=False, indent=2)
                logger.info(f"Instagram ì¿ í‚¤ íŒŒì¼ ì €ì¥: {list(cookies.keys())}")

            with InstagramCrawler(headless=use_headless, use_api=True, collect_comments=True) as crawler_instance:
                if cookies and crawler_instance.session:
                    apply_cookies_to_session(crawler_instance.session, cookies, domain)
                return crawler_instance.crawl_post(url)

        elif platform == "facebook":
            from src.crawlers.facebook_crawler import FacebookCrawler

            # ì‚¬ì´ë“œë°” ì¿ í‚¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (Selenium fallbackìš©)
            if cookies:
                cookie_file = Path(__file__).parent.parent / "data" / "cookies" / "facebook_cookies.json"
                cookie_file.parent.mkdir(parents=True, exist_ok=True)
                cookie_list = [
                    {"name": name, "value": value, "domain": ".facebook.com", "path": "/", "secure": True, "httpOnly": True}
                    for name, value in cookies.items()
                ]
                with open(cookie_file, "w", encoding="utf-8") as f:
                    json.dump(cookie_list, f, ensure_ascii=False, indent=2)
                logger.info(f"Facebook ì¿ í‚¤ íŒŒì¼ ì €ì¥: {list(cookies.keys())}")

            # Selenium ì‚¬ìš© (ì¿ í‚¤ íŒŒì¼ ìë™ ë¡œë“œ)
            with FacebookCrawler(headless=use_headless, use_api=False, use_scraper=False, use_mobile=False, collect_comments=False) as crawler_instance:
                result = crawler_instance.crawl_post(url)
                logger.info(f"Facebook ê²°ê³¼: likes={result.get('likes')}, comments={result.get('comments')}")
                return result

        elif platform == "xiaohongshu":
            from src.crawlers.xhs_crawler import XHSCrawler
            # use_api=Falseë¡œ QR ë¡œê·¸ì¸ ê°•ì œ (APIëŠ” ì˜ëª»ëœ ê²Œì‹œë¬¼ ë°˜í™˜ ê°€ëŠ¥)
            with XHSCrawler(headless=use_headless, use_api=False, collect_comments=True) as crawler_instance:
                if cookies and crawler_instance.session:
                    apply_cookies_to_session(crawler_instance.session, cookies, domain)
                return crawler_instance.crawl_post(url)

        elif platform == "dcard":
            from src.crawlers.dcard_crawler import DcardCrawler
            # Cloud í™˜ê²½ì—ì„œëŠ” headless ê°•ì œ, ë¡œì»¬ì—ì„œëŠ” use_headless ë”°ë¦„
            with DcardCrawler(headless=use_headless, use_api=True) as crawler_instance:
                if cookies and hasattr(crawler_instance, 'scraper'):
                    apply_cookies_to_session(crawler_instance.scraper, cookies, domain)
                return crawler_instance.crawl_post(url)

        # ê¸°ë³¸ ë™ì‘
        return crawler(url)

    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜ ({platform}, {url}): {e}")
        error_msg = str(e)
        if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
            safe_error = "ìš”ì²­ ì‹œê°„ ì´ˆê³¼"
        elif "connection" in error_msg.lower():
            safe_error = "ì„œë²„ ì—°ê²° ì‹¤íŒ¨"
        elif "cookie" in error_msg.lower() or "login" in error_msg.lower():
            safe_error = "ì¸ì¦ í•„ìš” - ì¿ í‚¤/ë¡œê·¸ì¸ í™•ì¸"
        else:
            safe_error = error_msg[:100] if len(error_msg) <= 100 else error_msg[:100] + "..."
        return {
            "platform": platform,
            "url": url,
            "error": safe_error,
            "crawled_at": datetime.now().isoformat(),
        }


def is_crawl_result_valid(result: dict) -> bool:
    """
    í¬ë¡¤ë§ ê²°ê³¼ê°€ ì‹¤ì œë¡œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸

    "ì„±ê³µ N / ì‹¤íŒ¨ 0" ë¬¸ì œ í•´ê²°:
    - ì—ëŸ¬ê°€ ì—†ì–´ë„ ì‹¤ì œ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬

    Args:
        result: í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

    Returns:
        ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ True
    """
    # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì‹¤íŒ¨
    if result.get("error"):
        return False

    # í”Œë«í¼ë³„ ìœ íš¨ì„± ê²€ì‚¬
    platform = result.get("platform", "")

    # í•„ìˆ˜ ë°ì´í„° í™•ì¸: ì‘ì„±ì ë˜ëŠ” ìƒí˜¸ì‘ìš© ìˆ˜ì¹˜ ì¤‘ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
    has_author = bool(result.get("author"))
    has_likes = (result.get("likes") or 0) > 0
    has_comments = (result.get("comments") or 0) > 0
    has_views = (result.get("views") or 0) > 0
    has_favorites = (result.get("favorites") or 0) > 0
    has_shares = (result.get("shares") or 0) > 0

    # YouTube: ì¡°íšŒìˆ˜ê°€ í•µì‹¬ ì§€í‘œ
    if platform == "youtube":
        return has_views or has_likes or has_comments

    # ìƒ¤ì˜¤í™ìŠˆ: ì¢‹ì•„ìš”ë‚˜ ì¦ê²¨ì°¾ê¸°ê°€ í•µì‹¬
    if platform == "xiaohongshu":
        return has_author or has_likes or has_favorites or has_comments

    # ì¸ìŠ¤íƒ€ê·¸ë¨: ì¢‹ì•„ìš”ê°€ í•µì‹¬
    if platform == "instagram":
        return has_author or has_likes or has_comments or has_views

    # í˜ì´ìŠ¤ë¶: ì¢‹ì•„ìš”ë‚˜ ê³µìœ ê°€ í•µì‹¬
    if platform == "facebook":
        return has_author or has_likes or has_shares or has_comments

    # Dcard: ì¢‹ì•„ìš” ë˜ëŠ” ëŒ“ê¸€
    if platform == "dcard":
        return has_author or has_likes or has_comments

    # ê¸°íƒ€ í”Œë«í¼: ìµœì†Œ í•˜ë‚˜ì˜ ì§€í‘œê°€ ìˆì–´ì•¼ í•¨
    return has_author or has_likes or has_comments or has_views


def get_crawl_failure_reason(result: dict) -> str:
    """
    í¬ë¡¤ë§ ì‹¤íŒ¨ ì´ìœ ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë©”ì‹œì§€ë¡œ ë°˜í™˜

    Args:
        result: í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

    Returns:
        ì‹¤íŒ¨ ì´ìœ  ë©”ì‹œì§€
    """
    error = result.get("error", "")
    error_type = result.get("error_type", "")
    platform = result.get("platform", "")

    # error_type ê¸°ë°˜ ë¶„ë¥˜ (í¬ë¡¤ëŸ¬ì—ì„œ ì„¤ì •)
    if error_type == "not_found":
        return "ê²Œì‹œë¬¼ì´ ì‚­ì œë˜ì—ˆê±°ë‚˜ ë¹„ê³µê°œ ìƒíƒœì…ë‹ˆë‹¤"
    if error_type == "validation_error":
        return "ì˜ëª»ëœ URL í˜•ì‹ì…ë‹ˆë‹¤"

    # ëª…ì‹œì  ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš° (ë¬¸ìì—´ ë§¤ì¹­ fallback)
    if error:
        if "ì‚­ì œ" in error or "ë¹„ê³µê°œ" in error or "not found" in error.lower():
            return "ê²Œì‹œë¬¼ì´ ì‚­ì œë˜ì—ˆê±°ë‚˜ ë¹„ê³µê°œ ìƒíƒœì…ë‹ˆë‹¤"
        if "timeout" in error.lower() or "ì‹œê°„" in error:
            return "í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ì´ˆê³¼"
        if "cookie" in error.lower() or "ì¿ í‚¤" in error or "ë¡œê·¸ì¸" in error:
            return "ì¿ í‚¤ ë§Œë£Œ ë˜ëŠ” ë¡œê·¸ì¸ í•„ìš”"
        if "qr" in error.lower():
            return "QR ì¸ì¦ í•„ìš”"
        if "rate" in error.lower() or "limit" in error.lower():
            return "ìš”ì²­ ì œí•œ (ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„)"
        if "cloud" in error.lower():
            return "ì„œë²„ í™˜ê²½ ì œí•œ (ë¡œì»¬ ì‹¤í–‰ ê¶Œì¥)"
        return error[:50]

    # ì—ëŸ¬ëŠ” ì—†ì§€ë§Œ ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
    has_any_data = (
        result.get("author") or
        result.get("likes", 0) > 0 or
        result.get("comments", 0) > 0 or
        result.get("views", 0) if result.get("views") is not None else False
    )

    if not has_any_data:
        if platform == "xiaohongshu":
            return "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (QR ì¸ì¦ ë˜ëŠ” ì¿ í‚¤ í•„ìš”)"
        if platform == "instagram":
            return "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¿ í‚¤ ë§Œë£Œ ë˜ëŠ” ë¹„ê³µê°œ ê³„ì •)"
        if platform == "facebook":
            return "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¿ í‚¤ ë§Œë£Œ ë˜ëŠ” ë¹„ê³µê°œ ê²Œì‹œë¬¼)"
        return "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (ì¸ì¦ í•„ìš”)"

    return ""


def get_platform_crawl_info(platform: str) -> dict:
    """
    í”Œë«í¼ë³„ í¬ë¡¤ë§ ì •ë³´ ë°˜í™˜

    Args:
        platform: í”Œë«í¼ ì´ë¦„

    Returns:
        ì˜ˆìƒ ì‹œê°„, ì¸ì¦ í•„ìš” ì—¬ë¶€ ë“± ì •ë³´
    """
    platform_info = {
        "xiaohongshu": {
            "display_name": "ìƒ¤ì˜¤í™ìŠˆ",
            "estimated_time": "15-30ì´ˆ",
            "requires_auth": True,
            "auth_type": "QR",
            "auth_message": "QR ì½”ë“œ ì¸ì¦ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì°½ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "tips": "ë¸Œë¼ìš°ì € ì°½ì—ì„œ QR ì½”ë“œê°€ ë‚˜íƒ€ë‚˜ë©´ ìƒ¤ì˜¤í™ìŠˆ ì•±ìœ¼ë¡œ ìŠ¤ìº”í•˜ì„¸ìš”.",
        },
        "instagram": {
            "display_name": "ì¸ìŠ¤íƒ€ê·¸ë¨",
            "estimated_time": "5-10ì´ˆ",
            "requires_auth": True,
            "auth_type": "Cookie",
            "auth_message": "ë¡œê·¸ì¸ ì¿ í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì¸ì¦ì„ ì„¤ì •í•˜ì„¸ìš”.",
            "tips": "ì¿ í‚¤ ì¸ì¦ì´ ë§Œë£Œë˜ë©´ ë‹¤ì‹œ ì„¤ì •í•´ì£¼ì„¸ìš”.",
        },
        "facebook": {
            "display_name": "í˜ì´ìŠ¤ë¶",
            "estimated_time": "5-10ì´ˆ",
            "requires_auth": True,
            "auth_type": "Cookie",
            "auth_message": "ë¡œê·¸ì¸ ì¿ í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
            "tips": "ê³µê°œ ê²Œì‹œë¬¼ë§Œ ìˆ˜ì§‘ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        },
        "youtube": {
            "display_name": "ìœ íŠœë¸Œ",
            "estimated_time": "3-5ì´ˆ",
            "requires_auth": False,
            "auth_type": None,
            "auth_message": None,
            "tips": "ê³µê°œ ë™ì˜ìƒì€ ì¸ì¦ ì—†ì´ ìˆ˜ì§‘ë©ë‹ˆë‹¤.",
        },
        "dcard": {
            "display_name": "Dcard",
            "estimated_time": "3-5ì´ˆ",
            "requires_auth": False,
            "auth_type": None,
            "auth_message": None,
            "tips": "ëŒ€ë§Œ ì»¤ë®¤ë‹ˆí‹° í”Œë«í¼ì…ë‹ˆë‹¤.",
        },
    }
    return platform_info.get(platform, {
        "display_name": platform,
        "estimated_time": "5-10ì´ˆ",
        "requires_auth": False,
        "auth_type": None,
        "auth_message": None,
        "tips": "",
    })


def run_crawling():
    """í¬ë¡¤ë§ ì‹¤í–‰"""
    urls = st.session_state.get("urls", [])
    valid_urls = [u for u in urls if u.get("valid")]

    if not valid_urls:
        st.error("í¬ë¡¤ë§í•  ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")
        st.session_state.crawling_status = "error"
        return

    # ë™ì¼ URL ì¤‘ë³µ ê²€ì¶œ ê²½ê³ 
    url_list = [u.get("url", "") for u in valid_urls]
    duplicates = [u for u in set(url_list) if url_list.count(u) > 1]
    if duplicates:
        st.warning(f"ë™ì¼í•œ URLì´ {len(duplicates)}ê±´ ì¤‘ë³µ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì¤‘ë³µ URLì€ ê°ê° ë³„ë„ë¡œ í¬ë¡¤ë§ë©ë‹ˆë‹¤.")
        logger.warning(f"ì¤‘ë³µ URL ê°ì§€: {duplicates}")

    logger.info(f"í¬ë¡¤ë§ ì‹œì‘: ì´ {len(valid_urls)}ê°œ URL ({len(set(url_list))}ê°œ ê³ ìœ )")

    st.markdown("### í¬ë¡¤ë§ ì§„í–‰ ì¤‘")

    # í”Œë«í¼ë³„ URL ì¹´ìš´íŠ¸ ë¶„ì„
    platform_counts = {}
    for u in valid_urls:
        p = u.get("platform", "unknown")
        platform_counts[p] = platform_counts.get(p, 0) + 1

    # ìƒ¤ì˜¤í™ìŠˆê°€ í¬í•¨ëœ ê²½ìš° ì¸ì¦ ëª¨ë“œ ìë™ í™œì„±í™” + QR ì¸ì¦ ì•ˆë‚´ í‘œì‹œ
    if "xiaohongshu" in platform_counts and IS_LOCAL:
        xhs_count = platform_counts["xiaohongshu"]
        # ì¸ì¦ ëª¨ë“œ ìë™ í™œì„±í™” (QR ìŠ¤ìº”ì„ ìœ„í•´ ë¸Œë¼ìš°ì € ì°½ í•„ìš”)
        if not st.session_state.get("auth_mode", False):
            st.session_state.auth_mode = True
            st.info(
                f"**ìƒ¤ì˜¤í™ìŠˆ URL ê°ì§€ ({xhs_count}ê°œ)** - ì¸ì¦ ëª¨ë“œê°€ ìë™ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                "QR ì½”ë“œ ìŠ¤ìº”ì„ ìœ„í•´ ë¸Œë¼ìš°ì € ì°½ì´ ì—´ë¦½ë‹ˆë‹¤."
            )
        st.warning(
            f"**ìƒ¤ì˜¤í™ìŠˆ QR ì¸ì¦ ì•ˆë‚´** ({xhs_count}ê°œ URL)\n\n"
            "ìƒ¤ì˜¤í™ìŠˆ í¬ë¡¤ë§ ì‹œ QR ì½”ë“œ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
            "- ë¸Œë¼ìš°ì € ì°½ì´ ì—´ë¦¬ë©´ QR ì½”ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”\n"
            "- ìƒ¤ì˜¤í™ìŠˆ ì•±ìœ¼ë¡œ QR ì½”ë“œë¥¼ ìŠ¤ìº”í•˜ì—¬ ì¸ì¦í•˜ì„¸ìš”\n"
            "- ì¸ì¦ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í¬ë¡¤ë§ì´ ì§„í–‰ë©ë‹ˆë‹¤"
        )

    # í”Œë«í¼ë³„ ì˜ˆìƒ ì‹œê°„ ì•ˆë‚´
    with st.expander("í”Œë«í¼ë³„ ì˜ˆìƒ ì†Œìš” ì‹œê°„", expanded=False):
        for platform, count in platform_counts.items():
            info = get_platform_crawl_info(platform)
            auth_badge = ""
            if info.get("requires_auth"):
                auth_type = info.get("auth_type", "")
                if auth_type == "QR":
                    auth_badge = " [QR ì¸ì¦ í•„ìš”]"
                elif auth_type == "Cookie":
                    auth_badge = " [ì¿ í‚¤ ì¸ì¦]"
            st.markdown(
                f"- **{info['display_name']}**: {count}ê°œ URL, "
                f"URLë‹¹ ì•½ {info['estimated_time']}{auth_badge}"
            )
            if info.get("tips"):
                st.caption(f"  Tip: {info['tips']}")

    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    platform_status = st.empty()  # í”Œë«í¼ë³„ ìƒíƒœ í‘œì‹œìš©
    result_container = st.container()

    total = len(valid_urls)

    # === ìƒ¤ì˜¤í™ìŠˆ ë°°ì¹˜ ì²˜ë¦¬ (í•œ ë²ˆì˜ ë¡œê·¸ì¸ìœ¼ë¡œ ëª¨ë“  URL ì²˜ë¦¬) ===
    xhs_urls = [u for u in valid_urls if u.get("platform") == "xiaohongshu"]
    other_urls = [u for u in valid_urls if u.get("platform") != "xiaohongshu"]
    xhs_results = {}  # URL -> result ë§¤í•‘

    if xhs_urls and IS_LOCAL:
        platform_info = get_platform_crawl_info("xiaohongshu")
        platform_status.info(
            f"**{platform_info['display_name']} ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘** ({len(xhs_urls)}ê°œ URL)\n\n"
            "QR ì½”ë“œ ì¸ì¦ì´ í•„ìš”í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì„ í™•ì¸í•˜ì„¸ìš”.\n"
            "**í•œ ë²ˆë§Œ ì¸ì¦í•˜ë©´ ëª¨ë“  URLì´ ì²˜ë¦¬ë©ë‹ˆë‹¤.**"
        )
        status_text.markdown(f"**ìƒ¤ì˜¤í™ìŠˆ ë°°ì¹˜ ì²˜ë¦¬ ì¤€ë¹„ ì¤‘...** ({len(xhs_urls)}ê°œ URL)")

        try:
            from src.crawlers.xhs_crawler import XHSCrawler

            with XHSCrawler(headless=False, use_api=False, collect_comments=True) as xhs_crawler:
                for idx, url_info in enumerate(xhs_urls):
                    url = url_info.get("url")
                    progress_bar.progress((idx + 1) / total)
                    status_text.markdown(
                        f"**ìƒ¤ì˜¤í™ìŠˆ í¬ë¡¤ë§ ì¤‘:** {idx + 1}/{len(xhs_urls)} - {url[:50]}..."
                    )

                    try:
                        # ì²« ë²ˆì§¸ URLì—ì„œë§Œ ë¡œê·¸ì¸ ì‹œë„
                        result = xhs_crawler.crawl_post(url, auto_login=(idx == 0))
                        xhs_results[url] = result

                        # ê²°ê³¼ í‘œì‹œ
                        is_valid = is_crawl_result_valid(result)
                        if is_valid:
                            metrics = []
                            if result.get("likes", 0) > 0:
                                metrics.append(f"ì¢‹ì•„ìš” {result['likes']}")
                            if result.get("comments", 0) > 0:
                                metrics.append(f"ëŒ“ê¸€ {result['comments']}")
                            metrics_str = ", ".join(metrics) if metrics else "ë°ì´í„° ìˆ˜ì§‘ë¨"
                            with result_container:
                                st.markdown(f"**ìƒ¤ì˜¤í™ìŠˆ {idx + 1}** - ì„±ê³µ ({metrics_str})")
                        else:
                            with result_container:
                                st.markdown(f"**ìƒ¤ì˜¤í™ìŠˆ {idx + 1}** - ì‹¤íŒ¨: {result.get('error', 'ë°ì´í„° ì—†ìŒ')[:50]}")

                        time.sleep(2)  # Rate limiting

                    except Exception as e:
                        logger.error(f"ìƒ¤ì˜¤í™ìŠˆ í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
                        xhs_results[url] = {
                            "platform": "xiaohongshu",
                            "url": url,
                            "error": str(e),
                            "crawled_at": datetime.now().isoformat(),
                        }

                platform_status.success(f"**ìƒ¤ì˜¤í™ìŠˆ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ!** ({len(xhs_urls)}ê°œ URL)")

        except Exception as e:
            logger.error(f"ìƒ¤ì˜¤í™ìŠˆ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            platform_status.error(f"**ìƒ¤ì˜¤í™ìŠˆ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜:** {str(e)}")
            # ì‹¤íŒ¨í•œ URLë“¤ì— ì—ëŸ¬ ê²°ê³¼ ì¶”ê°€
            for url_info in xhs_urls:
                url = url_info.get("url")
                if url not in xhs_results:
                    xhs_results[url] = {
                        "platform": "xiaohongshu",
                        "url": url,
                        "error": str(e),
                        "crawled_at": datetime.now().isoformat(),
                    }

    # === í”Œë«í¼ë³„ ë°°ì¹˜ í¬ë¡¤ë§ (ì„¸ì…˜ ì¬ì‚¬ìš©ìœ¼ë¡œ ì†ë„ ìµœì í™”) ===
    # ê°™ì€ í”Œë«í¼ì˜ URLì„ í•˜ë‚˜ì˜ ë¸Œë¼ìš°ì € ì„¸ì…˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë¸Œë¼ìš°ì € ì‹œì‘ ì˜¤ë²„í—¤ë“œ ì œê±°

    # í”Œë«í¼ë³„ URL ê·¸ë£¹í™” (ìƒ¤ì˜¤í™ìŠˆëŠ” ì´ë¯¸ ë°°ì¹˜ ì²˜ë¦¬ë¨)
    platform_url_groups = {}  # platform -> [url_info, ...]
    for url_info in valid_urls:
        p = url_info.get("platform")
        url = url_info.get("url")
        # ìƒ¤ì˜¤í™ìŠˆ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ë°”ë¡œ ì¶”ê°€
        if p == "xiaohongshu" and url in xhs_results:
            results.append(xhs_results[url])
            continue
        if p not in platform_url_groups:
            platform_url_groups[p] = []
        platform_url_groups[p].append(url_info)

    processed_count = len(xhs_results)
    auth_mode = st.session_state.get("auth_mode", False)

    # í”Œë«í¼ë³„ ë°°ì¹˜ ì²˜ë¦¬
    for platform, url_group in platform_url_groups.items():
        platform_info = get_platform_crawl_info(platform)

        # ë°°ì¹˜ í¬ë¡¤ë§ ì•ˆë‚´
        if len(url_group) > 1:
            platform_status.info(
                f"**{platform_info['display_name']} ë°°ì¹˜ í¬ë¡¤ë§** ({len(url_group)}ê°œ URL)\n\n"
                f"í•˜ë‚˜ì˜ ì„¸ì…˜ìœ¼ë¡œ ëª¨ë“  URLì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."
            )
        else:
            if platform_info.get("requires_auth"):
                platform_status.info(
                    f"**{platform_info['display_name']} í¬ë¡¤ë§ ì¤‘**\n\n"
                    f"{platform_info.get('auth_message', '')}"
                )
            else:
                platform_status.info(
                    f"**{platform_info['display_name']} í¬ë¡¤ë§ ì¤‘**\n\n"
                    f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: URLë‹¹ {platform_info['estimated_time']}"
                )

        # ì¸ì¦ ì•ˆë‚´
        if platform == "dcard" and auth_mode:
            platform_status.warning(
                f"**{platform_info['display_name']} - Cloudflare ì¸ì¦ ëŒ€ê¸° ì¤‘...**\n\n"
                "ë¸Œë¼ìš°ì € ì°½ì— Cloudflare ì¸ì¦ì´ ë‚˜íƒ€ë‚˜ë©´ ì™„ë£Œí•´ì£¼ì„¸ìš”.\n"
                "ì¸ì¦ í›„ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤."
            )
        elif platform == "xiaohongshu" and auth_mode:
            platform_status.warning(
                f"**{platform_info['display_name']} - QR ì¸ì¦ ëŒ€ê¸° ì¤‘...**\n\n"
                "ë¸Œë¼ìš°ì € ì°½ì— QR ì½”ë“œê°€ ë‚˜íƒ€ë‚˜ë©´ ìƒ¤ì˜¤í™ìŠˆ ì•±ìœ¼ë¡œ ìŠ¤ìº”í•˜ì„¸ìš”.\n"
                "ì¸ì¦ í›„ ìë™ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤."
            )

        # YouTube ë° ê¸°íƒ€ í”Œë«í¼: ì„¸ì…˜ ë¶ˆí•„ìš”, ê¸°ì¡´ ë°©ì‹ ìœ ì§€
        if platform not in ["facebook", "instagram", "dcard"]:
            for url_info in url_group:
                url = url_info.get("url")
                processed_count += 1
                progress_bar.progress(processed_count / total)
                status_text.markdown(
                    f"**ì§„í–‰ ì¤‘:** {processed_count}/{total} - "
                    f"{platform_info['display_name']} - {url[:50]}..."
                )
                try:
                    result = crawl_with_cookies(platform, url, auth_mode=auth_mode)
                    results.append(result)
                    is_valid = is_crawl_result_valid(result)
                    if is_valid:
                        metrics = []
                        if result.get("likes", 0) > 0:
                            metrics.append(f"ì¢‹ì•„ìš” {result['likes']}")
                        if result.get("comments", 0) > 0:
                            metrics.append(f"ëŒ“ê¸€ {result['comments']}")
                        if result.get("views") and result.get("views", 0) > 0:
                            metrics.append(f"ì¡°íšŒìˆ˜ {result['views']}")
                        metrics_str = ", ".join(metrics) if metrics else "ë°ì´í„° ìˆ˜ì§‘ë¨"
                        with result_container:
                            st.markdown(
                                f"**{processed_count}. {platform_info['display_name']}** - ì„±ê³µ ({metrics_str})"
                            )
                    else:
                        failure_reason = get_crawl_failure_reason(result)
                        if not result.get("error"):
                            result["error"] = failure_reason or "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨"
                        with result_container:
                            st.markdown(
                                f"**{processed_count}. {platform_info['display_name']}** - "
                                f"ì‹¤íŒ¨: {failure_reason or result.get('error', '')[:50]}"
                            )
                        if platform == "xiaohongshu":
                            error_msg = f"**{platform_info['display_name']} - ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨**\n\n"
                            if IS_LOCAL and not auth_mode:
                                error_msg += "**í•´ê²° ë°©ë²•:**\n"
                                error_msg += "1. ì‚¬ì´ë“œë°”ì—ì„œ 'ì¸ì¦ ëª¨ë“œ'ë¥¼ í™œì„±í™”í•˜ì„¸ìš”\n"
                                error_msg += "2. ë¸Œë¼ìš°ì € ì°½ì—ì„œ QR ì½”ë“œë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”\n"
                                error_msg += "ë˜ëŠ” ì¿ í‚¤ë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”."
                            else:
                                error_msg += "QR ì¸ì¦ì´ í•„ìš”í•˜ê±°ë‚˜ ì¿ í‚¤ê°€ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                                error_msg += "ì‚¬ì´ë“œë°”ì—ì„œ ì¿ í‚¤ë¥¼ ë‹¤ì‹œ ì„¤ì •í•´ì£¼ì„¸ìš”."
                            platform_status.error(error_msg)
                except Exception as e:
                    logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜ ({url}): {e}")
                    results.append({
                        "platform": platform, "url": url,
                        "error": str(e), "crawled_at": datetime.now().isoformat(),
                    })
                    with result_container:
                        st.markdown(
                            f"**{processed_count}. {platform_info['display_name']}** - ì˜¤ë¥˜: {str(e)[:50]}"
                        )
                time.sleep(1)
            continue

        # === Facebook, Instagram, Dcard: ì„¸ì…˜ ì¬ì‚¬ìš© ë°°ì¹˜ í¬ë¡¤ë§ ===
        cookies = get_platform_cookies(platform)
        domain = PLATFORM_DOMAINS.get(platform, "")
        has_cookies = bool(cookies)

        # headless ëª¨ë“œ ê²°ì • (Cloud í™˜ê²½ ìš°ì„ )
        if IS_CLOUD:
            use_headless = True
        elif platform == "dcard" and not auth_mode:
            use_headless = False  # Dcard: Cloudflare ìš°íšŒ ìœ„í•´ ë¸Œë¼ìš°ì € í‘œì‹œ (ë¡œì»¬ë§Œ)
        elif auth_mode:
            use_headless = False
        elif has_cookies:
            use_headless = True
        else:
            use_headless = True

        # ì¿ í‚¤ íŒŒì¼ ì €ì¥ (ì„¸ì…˜ ì‹œì‘ ì „ í•œ ë²ˆë§Œ)
        if cookies and platform in ["facebook", "instagram"]:
            cookie_domain = ".facebook.com" if platform == "facebook" else ".instagram.com"
            cookie_file = Path(__file__).parent.parent / "data" / "cookies" / f"{platform}_cookies.json"
            cookie_file.parent.mkdir(parents=True, exist_ok=True)
            cookie_list = [
                {"name": name, "value": value, "domain": cookie_domain, "path": "/", "secure": True, "httpOnly": True}
                for name, value in cookies.items()
            ]
            with open(cookie_file, "w", encoding="utf-8") as f:
                json.dump(cookie_list, f, ensure_ascii=False, indent=2)
            logger.info(f"{platform} ì¿ í‚¤ íŒŒì¼ ì €ì¥ ì™„ë£Œ")

        try:
            # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í”Œë«í¼ë‹¹ 1íšŒ)
            if platform == "facebook":
                from src.crawlers.facebook_crawler import FacebookCrawler
                crawler_instance = FacebookCrawler(
                    headless=use_headless, use_api=False, use_scraper=False,
                    use_mobile=False, collect_comments=False
                )
            elif platform == "instagram":
                from src.crawlers.instagram_crawler import InstagramCrawler
                crawler_instance = InstagramCrawler(
                    headless=use_headless, use_api=True, collect_comments=True
                )
            else:  # dcard
                from src.crawlers.dcard_crawler import DcardCrawler
                crawler_instance = DcardCrawler(headless=use_headless, use_api=True)

            with crawler_instance as c:
                # ì¿ í‚¤ ì ìš© (ì„¸ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
                if platform == "instagram" and cookies and c.session:
                    apply_cookies_to_session(c.session, cookies, domain)
                elif platform == "dcard" and cookies and hasattr(c, 'scraper'):
                    apply_cookies_to_session(c.scraper, cookies, domain)

                for url_idx, url_info in enumerate(url_group):
                    url = url_info.get("url")
                    processed_count += 1
                    progress_bar.progress(processed_count / total)
                    status_text.markdown(
                        f"**ì§„í–‰ ì¤‘:** {processed_count}/{total} - "
                        f"{platform_info['display_name']} ({url_idx + 1}/{len(url_group)}) - {url[:50]}..."
                    )

                    try:
                        result = c.crawl_post(url)

                        # ê°™ì€ ì‘ì„±ì ë‹¤ë¥¸ ê²Œì‹œë¬¼ ë®ì–´ì“°ê¸° ë°©ì§€: ì´ì „ ê²°ê³¼ì™€ ì™„ì „ ë™ì¼ ë°ì´í„°ì¸ì§€ í™•ì¸
                        if results and result.get("author") and not result.get("error"):
                            prev_same_author = [
                                r for r in results
                                if r.get("author") == result.get("author")
                                and r.get("platform") == result.get("platform")
                                and r.get("url") != result.get("url")
                                and r.get("likes") == result.get("likes")
                                and r.get("comments") == result.get("comments")
                            ]
                            if prev_same_author:
                                logger.warning(
                                    f"ë™ì¼ ì‘ì„±ì ë°ì´í„° ì¤‘ë³µ ê°ì§€: {result.get('author')}, URL={url[:50]}, "
                                    f"likes={result.get('likes')}, comments={result.get('comments')} â†’ ì¬í¬ë¡¤ë§ ì‹œë„"
                                )
                                time.sleep(3)  # CDN ìºì‹œ ê°±ì‹  ëŒ€ê¸°
                                result = c.crawl_post(url)

                        results.append(result)
                        logger.info(f"{platform} ë°°ì¹˜ ê²°ê³¼: likes={result.get('likes')}, comments={result.get('comments')}")

                        # ê²°ê³¼ í‘œì‹œ
                        is_valid = is_crawl_result_valid(result)
                        failure_reason = get_crawl_failure_reason(result) if not is_valid else ""

                        if not is_valid:
                            if not result.get("error"):
                                result["error"] = failure_reason or "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨"
                            with result_container:
                                st.markdown(
                                    f"**{processed_count}. {platform_info['display_name']}** - "
                                    f"ì‹¤íŒ¨: {failure_reason or result.get('error', '')[:50]}"
                                )
                            # ì‹¤íŒ¨ ì•ˆë‚´
                            if platform == "dcard":
                                error_msg = f"**{platform_info['display_name']} - ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨**\n\n"
                                if IS_LOCAL and not auth_mode:
                                    error_msg += "**í•´ê²° ë°©ë²•:**\n"
                                    error_msg += "1. ì‚¬ì´ë“œë°”ì—ì„œ 'ì¸ì¦ ëª¨ë“œ'ë¥¼ í™œì„±í™”í•˜ì„¸ìš”\n"
                                    error_msg += "2. ë¸Œë¼ìš°ì € ì°½ì—ì„œ Cloudflare ì¸ì¦ì„ ì™„ë£Œí•˜ì„¸ìš”\n"
                                else:
                                    error_msg += "Cloudflare ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
                                    error_msg += "ë¡œì»¬ í™˜ê²½ì—ì„œ ì¸ì¦ ëª¨ë“œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
                                platform_status.error(error_msg)
                            elif platform in ["instagram", "facebook"]:
                                platform_status.warning(
                                    f"**{platform_info['display_name']} - ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨**\n\n"
                                    "ì¿ í‚¤ê°€ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                                    "ì‚¬ì´ë“œë°”ì—ì„œ ì¿ í‚¤ë¥¼ ë‹¤ì‹œ ì„¤ì •í•´ì£¼ì„¸ìš”."
                                )
                        else:
                            with result_container:
                                metrics = []
                                if result.get("likes", 0) > 0:
                                    metrics.append(f"ì¢‹ì•„ìš” {result['likes']}")
                                if result.get("comments", 0) > 0:
                                    metrics.append(f"ëŒ“ê¸€ {result['comments']}")
                                if result.get("views") and result.get("views", 0) > 0:
                                    metrics.append(f"ì¡°íšŒìˆ˜ {result['views']}")
                                metrics_str = ", ".join(metrics) if metrics else "ë°ì´í„° ìˆ˜ì§‘ë¨"
                                st.markdown(
                                    f"**{processed_count}. {platform_info['display_name']}** - ì„±ê³µ ({metrics_str})"
                                )

                    except Exception as e:
                        logger.error(f"ë°°ì¹˜ í¬ë¡¤ë§ ì˜¤ë¥˜ ({platform}, {url}): {e}")
                        results.append({
                            "platform": platform, "url": url,
                            "error": str(e), "crawled_at": datetime.now().isoformat(),
                        })
                        with result_container:
                            st.markdown(
                                f"**{processed_count}. {platform_info['display_name']}** - ì˜¤ë¥˜: {str(e)[:50]}"
                            )

                    # í”Œë«í¼ë³„ ë”œë ˆì´
                    if platform == "dcard":
                        time.sleep(8)  # Cloudflare ì°¨ë‹¨ ë°©ì§€
                    elif platform in ["instagram", "facebook"]:
                        time.sleep(3)
                    else:
                        time.sleep(1)

            platform_status.success(
                f"**{platform_info['display_name']} ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ!** ({len(url_group)}ê°œ URL)"
            )

        except Exception as e:
            logger.error(f"{platform} ì„¸ì…˜ ì˜¤ë¥˜, ê°œë³„ í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜: {e}")
            platform_status.warning(
                f"**{platform_info['display_name']} - ì„¸ì…˜ ì˜¤ë¥˜, ê°œë³„ í¬ë¡¤ë§ìœ¼ë¡œ ì „í™˜**"
            )
            # ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ URLë“¤ì€ ê°œë³„ í¬ë¡¤ë§ìœ¼ë¡œ fallback
            processed_urls = {r.get("url") for r in results}
            for url_info in url_group:
                url = url_info.get("url")
                if url not in processed_urls:
                    processed_count += 1
                    progress_bar.progress(processed_count / total)
                    status_text.markdown(
                        f"**ì§„í–‰ ì¤‘ (ê°œë³„):** {processed_count}/{total} - "
                        f"{platform_info['display_name']} - {url[:50]}..."
                    )
                    try:
                        result = crawl_with_cookies(platform, url, auth_mode=auth_mode)
                        results.append(result)
                    except Exception as fallback_e:
                        logger.error(f"ê°œë³„ í¬ë¡¤ë§ë„ ì‹¤íŒ¨ ({url}): {fallback_e}")
                        results.append({
                            "platform": platform, "url": url,
                            "error": str(fallback_e), "crawled_at": datetime.now().isoformat(),
                        })
                    is_valid = is_crawl_result_valid(results[-1])
                    with result_container:
                        st.markdown(
                            f"**{processed_count}. {platform_info['display_name']}** - "
                            f"{'ì„±ê³µ' if is_valid else 'ì‹¤íŒ¨: ' + results[-1].get('error', '')[:30]}"
                        )

    # ê²°ê³¼ ì €ì¥
    st.session_state.crawl_results = results
    st.session_state.crawling_status = "completed"

    progress_bar.progress(1.0)
    status_text.markdown("**í¬ë¡¤ë§ ì™„ë£Œ!**")
    platform_status.empty()  # í”Œë«í¼ ìƒíƒœ ë©”ì‹œì§€ ì œê±°

    # ìµœì¢… ê²°ê³¼ ìš”ì•½ - ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì—¬ë¶€ ê¸°ì¤€
    success_count = sum(1 for r in results if is_crawl_result_valid(r))
    error_count = len(results) - success_count

    # ì‹¤íŒ¨í•œ URLë“¤ì˜ í”Œë«í¼ë³„ ë¶„ì„
    failed_by_platform = {}
    for r in results:
        if not is_crawl_result_valid(r):
            p = r.get("platform", "unknown")
            if p not in failed_by_platform:
                failed_by_platform[p] = []
            failed_by_platform[p].append(get_crawl_failure_reason(r))

    if error_count == 0:
        st.success(f"í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. {len(results)}ê°œ URL ëª¨ë‘ ì„±ê³µ!")
    else:
        st.warning(
            f"í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. "
            f"ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {error_count}ê°œ"
        )
        # ì‹¤íŒ¨ ì›ì¸ ìƒì„¸ ì•ˆë‚´
        if failed_by_platform:
            with st.expander("ì‹¤íŒ¨ ì›ì¸ ìƒì„¸", expanded=True):
                for platform, reasons in failed_by_platform.items():
                    info = get_platform_crawl_info(platform)
                    unique_reasons = list(set(reasons))
                    st.markdown(f"**{info['display_name']}** ({len(reasons)}ê±´ ì‹¤íŒ¨)")
                    for reason in unique_reasons:
                        st.markdown(f"  - {reason}")
                st.info(
                    "**í•´ê²° ë°©ë²•:**\n"
                    "1. ì‚¬ì´ë“œë°”ì—ì„œ í•´ë‹¹ í”Œë«í¼ì˜ ì¿ í‚¤ë¥¼ ë‹¤ì‹œ ì„¤ì •í•˜ì„¸ìš”.\n"
                    "2. ìƒ¤ì˜¤í™ìŠˆëŠ” QR ì¸ì¦ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                    "3. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                )

    st.rerun()


def retry_failed_crawls():
    """ì‹¤íŒ¨í•œ í•­ëª©ë§Œ ì¬ìˆ˜ì§‘"""
    results = st.session_state.get("crawl_results", [])

    # ì‹¤íŒ¨í•œ í•­ëª©ê³¼ ì„±ê³µí•œ í•­ëª© ë¶„ë¦¬
    failed_items = []
    successful_items = []

    for r in results:
        if is_crawl_result_valid(r):
            successful_items.append(r)
        else:
            failed_items.append(r)

    if not failed_items:
        st.warning("ì¬ìˆ˜ì§‘í•  ì‹¤íŒ¨ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.markdown(f"### ì‹¤íŒ¨ í•­ëª© ì¬ìˆ˜ì§‘ ì¤‘ ({len(failed_items)}ê±´)")

    progress_bar = st.progress(0)
    status_text = st.empty()
    result_container = st.container()

    retry_results = []
    auth_mode = st.session_state.get("auth_mode", False)

    for i, failed_item in enumerate(failed_items):
        url = failed_item.get("url")
        platform = failed_item.get("platform")
        platform_info = get_platform_crawl_info(platform)

        status_text.markdown(
            f"**ì¬ìˆ˜ì§‘ ì¤‘:** {i + 1}/{len(failed_items)} - "
            f"{platform_info['display_name']} - {url[:50]}..."
        )
        progress_bar.progress((i + 1) / len(failed_items))

        try:
            # ì¿ í‚¤ë¥¼ í¬í•¨í•˜ì—¬ ì¬í¬ë¡¤ë§
            result = crawl_with_cookies(platform, url, auth_mode=auth_mode)
            retry_results.append(result)

            is_valid = is_crawl_result_valid(result)
            failure_reason = get_crawl_failure_reason(result) if not is_valid else ""

            with result_container:
                if is_valid:
                    metrics = []
                    if result.get("likes", 0) > 0:
                        metrics.append(f"ì¢‹ì•„ìš” {result['likes']}")
                    if result.get("comments", 0) > 0:
                        metrics.append(f"ëŒ“ê¸€ {result['comments']}")
                    metrics_str = ", ".join(metrics) if metrics else "ë°ì´í„° ìˆ˜ì§‘ë¨"
                    st.markdown(
                        f"**{i + 1}. {platform_info['display_name']}** - ì¬ìˆ˜ì§‘ ì„±ê³µ ({metrics_str})"
                    )
                else:
                    st.markdown(
                        f"**{i + 1}. {platform_info['display_name']}** - "
                        f"ì¬ìˆ˜ì§‘ ì‹¤íŒ¨: {failure_reason or 'ë°ì´í„° ì—†ìŒ'}"
                    )

            # í”Œë«í¼ë³„ ë”œë ˆì´
            if platform in ["xiaohongshu", "instagram", "facebook"]:
                time.sleep(3)
            else:
                time.sleep(1)

        except Exception as e:
            logger.error(f"ì¬ìˆ˜ì§‘ ì˜¤ë¥˜ ({url}): {e}")
            retry_results.append({
                "platform": platform,
                "url": url,
                "error": str(e),
                "crawled_at": datetime.now().isoformat(),
            })
            with result_container:
                st.markdown(
                    f"**{i + 1}. {platform_info['display_name']}** - "
                    f"ì˜¤ë¥˜: {str(e)[:50]}"
                )

    # ê¸°ì¡´ ì„±ê³µ í•­ëª© + ì¬ìˆ˜ì§‘ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
    st.session_state.crawl_results = successful_items + retry_results

    progress_bar.progress(1.0)
    status_text.markdown("**ì¬ìˆ˜ì§‘ ì™„ë£Œ!**")

    # ì¬ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
    retry_success = sum(1 for r in retry_results if is_crawl_result_valid(r))
    retry_fail = len(retry_results) - retry_success

    if retry_fail == 0:
        st.success(f"ì¬ìˆ˜ì§‘ ì™„ë£Œ! {retry_success}ê±´ ëª¨ë‘ ì„±ê³µ")
    else:
        st.warning(f"ì¬ìˆ˜ì§‘ ì™„ë£Œ. ì„±ê³µ: {retry_success}ê±´, ì‹¤íŒ¨: {retry_fail}ê±´")

    time.sleep(1)
    st.rerun()


def render_results():
    """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
    results = st.session_state.get("crawl_results", [])

    if not results:
        return

    st.markdown("### í¬ë¡¤ë§ ê²°ê³¼")

    # ì‹¤íŒ¨í•œ í•­ëª© í™•ì¸
    failed_items = [r for r in results if not is_crawl_result_valid(r)]

    # ì¬ìˆ˜ì§‘ ë²„íŠ¼ (ì‹¤íŒ¨ í•­ëª©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í‘œì‹œ)
    if failed_items:
        col1, col2, col3 = st.columns([2, 2, 2])
        with col2:
            if st.button(
                f"ì‹¤íŒ¨ í•­ëª© ì¬ìˆ˜ì§‘ ({len(failed_items)}ê±´)",
                key="retry_failed",
                use_container_width=True,
                type="secondary",
            ):
                retry_failed_crawls()
                return

    # ê²°ê³¼ ì§‘ê³„
    aggregated = aggregate_results(results)
    grouped = group_by_platform(results)

    # ìš”ì•½ ì§€í‘œ ì¹´ë“œ
    st.markdown("#### ìº í˜ì¸ ì´ ì§€í‘œ")
    cols = st.columns(5)

    metrics = [
        ("ì´ ê²Œì‹œë¬¼", aggregated["total_posts"]),
        ("ì„±ê³µ", aggregated["success_count"]),
        ("ì´ ì¢‹ì•„ìš”", format_number(aggregated["total_likes"])),
        ("ì´ ëŒ“ê¸€", format_number(aggregated["total_comments"])),
        ("ì´ ì¡°íšŒìˆ˜", format_number(aggregated["total_views"])),
    ]

    for i, (label, value) in enumerate(metrics):
        with cols[i]:
            st.metric(label, value)

    # ì¶”ê°€ ì§€í‘œ
    cols2 = st.columns(4)
    with cols2[0]:
        st.metric("ì´ ê³µìœ ", format_number(aggregated["total_shares"]))
    with cols2[1]:
        st.metric("ì´ ì¦ê²¨ì°¾ê¸°", format_number(aggregated.get("total_favorites", 0)))
    with cols2[2]:
        st.metric("í‰ê·  ì¢‹ì•„ìš”", format_number(int(aggregated["avg_likes"])))
    with cols2[3]:
        st.metric("í‰ê·  ì¸ê²Œì´ì§€ë¨¼íŠ¸", format_number(int(aggregated.get("avg_engagement", 0))))

    # í”Œë«í¼ë³„ ìš”ì•½ í…Œì´ë¸”
    st.markdown("---")
    st.markdown("#### í”Œë«í¼ë³„ ìš”ì•½")

    summary_df = generate_summary_table(results)
    if not summary_df.empty:
        st.dataframe(summary_df, use_container_width=True, hide_index=True)

    # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
    st.markdown("---")
    st.markdown("#### ìƒì„¸ ê²°ê³¼")

    df = export_to_dataframe(results)

    # ì¡°íšŒìˆ˜ í‘œì‹œ - ì‹¤ì œ ë°ì´í„° ìœ ë¬´ ê¸°ë°˜ (v1.5.8)
    def format_views_display(row):
        """ì¡°íšŒìˆ˜ í‘œì‹œ (ë°ì´í„° ìœ ë¬´ë¡œ íŒë‹¨: None/NaN=ìˆ˜ì§‘ë¶ˆê°€, 0=-, ìˆ«ì=í¬ë§·íŒ…)"""
        views = row.get("views")
        if views is None or pd.isna(views):
            return "ìˆ˜ì§‘ ë¶ˆê°€"
        views = int(views)
        if views == 0:
            return "-"
        return format_number(views)

    # views ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    df["views"] = df.apply(format_views_display, axis=1)

    # ì»¬ëŸ¼ëª… í•œê¸€í™”
    column_rename = {
        "platform": "í”Œë«í¼",
        "url": "URL",
        "author": "ì‘ì„±ì",
        "title": "ì œëª©",
        "content": "ë‚´ìš©",
        "likes": "ì¢‹ì•„ìš”",
        "comments": "ëŒ“ê¸€",
        "shares": "ê³µìœ ",
        "views": "ì¡°íšŒìˆ˜",
        "favorites": "ì¦ê²¨ì°¾ê¸°",
        "crawled_at": "ìˆ˜ì§‘ì‹œê°„",
        "error": "ì˜¤ë¥˜",
    }
    df_display = df.rename(columns=column_rename)

    st.dataframe(
        df_display,
        use_container_width=True,
        hide_index=True,
        column_config={
            "URL": st.column_config.TextColumn("URL", width="small"),
            "ì œëª©": st.column_config.TextColumn("ì œëª©", width="medium"),
            "ë‚´ìš©": st.column_config.TextColumn("ë‚´ìš©", width="medium"),
            "ìˆ˜ì§‘ì‹œê°„": st.column_config.TextColumn("ìˆ˜ì§‘ì‹œê°„", width="small"),
        },
    )

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    st.markdown("---")
    col1, col2, col3 = st.columns([1, 1, 1])

    with col1:
        # CSV ë‹¤ìš´ë¡œë“œ
        csv = df.to_csv(index=False).encode('utf-8-sig')
        campaign_name = st.session_state.campaign_info.get("name", "campaign")
        filename = f"{campaign_name}_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        st.download_button(
            label="CSV ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name=filename,
            mime="text/csv",
            use_container_width=True,
        )

    with col2:
        # Excel ë‹¤ìš´ë¡œë“œ (ê²°ê³¼ + ìš”ì•½ + ëŒ“ê¸€ ì‹œíŠ¸ í¬í•¨)
        try:
            from io import BytesIO
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='ê²°ê³¼')
                summary_df.to_excel(writer, index=False, sheet_name='ìš”ì•½')

                # ëŒ“ê¸€ ì‹œíŠ¸ ì¶”ê°€
                comments_rows = []
                for r in results:
                    if r.get("comments_list"):
                        for c in r["comments_list"]:
                            comments_rows.append({
                                "í”Œë«í¼": r.get("platform", ""),
                                "ê²Œì‹œë¬¼URL": r.get("url", ""),
                                "ê²Œì‹œë¬¼ì‘ì„±ì": r.get("author", ""),
                                "ëŒ“ê¸€ì‘ì„±ì": c.get("author", ""),
                                "ëŒ“ê¸€ë‚´ìš©": c.get("text", ""),
                                "ì¢‹ì•„ìš”": c.get("likes", 0),
                            })
                if comments_rows:
                    comments_df = pd.DataFrame(comments_rows)
                    comments_df.to_excel(writer, index=False, sheet_name='ëŒ“ê¸€')

            excel_data = output.getvalue()

            st.download_button(
                label="Excel ë‹¤ìš´ë¡œë“œ",
                data=excel_data,
                file_name=filename.replace(".csv", ".xlsx"),
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True,
            )
        except ImportError:
            st.info("Excel ë‹¤ìš´ë¡œë“œëŠ” openpyxl ì„¤ì¹˜ í•„ìš”")

    with col3:
        # PDF ìƒì„± ë²„íŠ¼
        # ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§ (ì—ëŸ¬ ì—†ëŠ” ê²ƒ)
        valid_results = [r for r in results if is_crawl_result_valid(r)]

        if not valid_results:
            st.button(
                "PDF ë¦¬í¬íŠ¸ ìƒì„±",
                use_container_width=True,
                type="primary",
                disabled=True,
                help="ìœ íš¨í•œ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”."
            )
            st.caption("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ PDFë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            try:
                from src.report import generate_pdf_report

                # PDF ìƒì„± ì‹œë„
                campaign_info = st.session_state.campaign_info
                try:
                    pdf_bytes = generate_pdf_report(
                        campaign_name=campaign_info.get("name", "ìº í˜ì¸") or "ìº í˜ì¸",
                        advertiser_name=campaign_info.get("advertiser", "ê´‘ê³ ì£¼") or "ê´‘ê³ ì£¼",
                        start_date=str(campaign_info.get("start_date", "")) or datetime.now().strftime("%Y-%m-%d"),
                        end_date=str(campaign_info.get("end_date", "")) or datetime.now().strftime("%Y-%m-%d"),
                        results=valid_results  # ìœ íš¨í•œ ê²°ê³¼ë§Œ ì‚¬ìš©
                    )

                    pdf_filename = f"{campaign_info.get('name', 'campaign') or 'campaign'}_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"

                    st.download_button(
                        label=f"PDF ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ ({len(valid_results)}ê±´)",
                        data=pdf_bytes,
                        file_name=pdf_filename,
                        mime="application/pdf",
                        use_container_width=True,
                        type="primary",
                    )
                except Exception as gen_error:
                    logger.error(f"PDF ìƒì„± ì˜¤ë¥˜: {gen_error}")
                    if st.button("PDF ë¦¬í¬íŠ¸ ìƒì„±", use_container_width=True, type="primary"):
                        st.error(f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(gen_error)[:100]}")
                        st.info(
                            "**í•´ê²° ë°©ë²•:**\n"
                            "1. reportlabì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
                            "2. í•œê¸€ í°íŠ¸(ë§‘ì€ ê³ ë”•)ê°€ ì‹œìŠ¤í…œì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
                            "3. í¬ë¡¤ë§ ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”."
                        )

            except ImportError as e:
                if st.button("PDF ë¦¬í¬íŠ¸ ìƒì„±", use_container_width=True, type="primary"):
                    st.error(f"PDF ìƒì„± ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                    st.info(
                        "**ì„¤ì¹˜ ë°©ë²•:**\n"
                        "```\npip install reportlab\n```\n"
                        "ë˜ëŠ”\n"
                        "```\npip install weasyprint\n```"
                    )


def render_main_content():
    """ë©”ì¸ ì½˜í…ì¸  ë Œë”ë§"""
    # í—¤ë”
    st.markdown('<p class="main-header">ì¸í”Œë£¨ì–¸ì„œ ìº í˜ì¸ ì„±ê³¼ ë¦¬í¬íŠ¸</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">ë©€í‹° í”Œë«í¼ ê²Œì‹œë¬¼ ì„±ê³¼ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.</p>', unsafe_allow_html=True)

    # ìƒíƒœì— ë”°ë¥¸ ì½˜í…ì¸  í‘œì‹œ
    crawling_status = st.session_state.get("crawling_status", "idle")

    if crawling_status == "running":
        run_crawling()

    elif crawling_status == "completed":
        render_results()

        # ìƒˆë¡œ ì‹œì‘ ë²„íŠ¼
        st.markdown("---")
        if st.button("ìƒˆ ìº í˜ì¸ ì‹œì‘", use_container_width=False):
            st.session_state.urls = []
            st.session_state.crawl_results = []
            st.session_state.crawling_status = "idle"
            st.rerun()

    else:
        # URL ì…ë ¥ ë° ë¯¸ë¦¬ë³´ê¸°
        render_url_input()

        if st.session_state.get("urls"):
            st.markdown("---")
            render_url_preview()


def main():
    """ë©”ì¸ ì•± ì‹¤í–‰"""
    init_app_state()

    # ì¸ì¦ í™•ì¸
    if not is_authenticated():
        # ë¡œê·¸ì¸ í˜ì´ì§€
        st.markdown("### ì¸í”Œë£¨ì–¸ì„œ ìº í˜ì¸ ì„±ê³¼ ë¦¬í¬íŠ¸")
        st.markdown("---")
        show_login_form()
        return

    # ì‚¬ì´ë“œë°”
    render_sidebar()

    # ë©”ì¸ ì½˜í…ì¸ 
    render_main_content()


if __name__ == "__main__":
    main()
